# The Scientific Learning Engine

Cold Open — Experiment Standup
“Safety hit two out of ten last night,” Ana says. “We end the experiment if it drops again.”
“No heroics,” Mo replies. “We learn or we stop.” The whiteboard squeaks: PROBLEM, HYPOTHESIS, WINDOW.

Artifact: Experiment Card (template)

- Problem: what pattern hurts? (1–2 lines)
- Hypothesis: if we do X, Y will improve by Z% in N weeks
- Metrics: primary, secondary; Guardrails: stop conditions
- Window: start → end dates; Sample: who/where
- Decision: keep / refine / retire; Evidence: 3 bullets; Links: artifacts


The Ark only works if it learns. This chapter turns our culture of “try things and talk about them” into a repeatable learning engine that any Hub can run.

## Principles

- Radical transparency: publish anonymized Health Compass snapshots and decision logs
- Hypothesis‑driven: time‑bound experiments with explicit success and stop criteria
- Federated comparison: cross‑hub A/B governance with annual synthesis
- Pattern Library stewardship: retire, refine, or replicate based on evidence

## The Workflow (7 Steps)
1) Frame the hypothesis

- Example: “If we add a 15‑minute conflict‑repair ritual at the start of Flotilla meetings, we will reduce unresolved conflicts by 40% over 8 weeks.”

2) Define success metrics and guardrails

- Primary: Conflict Repair Success Rate (post‑meeting self‑reports)
- Secondary: Belonging delta; Meeting attendance stability
- Guardrails: No participant reports >2/10 safety; stop if violated

3) Baseline and consent

- Capture 2–4 weeks of baseline Compass signals; obtain explicit consent for measurement and publication (anonymized)

4) Run the experiment (4–12 weeks)

- Keep a one‑page log: date, people, intervention, anomalies, quick notes

5) Review and decide

- Did primary metric meet threshold? If yes, promote to “Candidate Pattern;” if no, archive with learnings; if mixed, refine and rerun

6) Publish the Experiment Card

- Problem, Hypothesis, Setup, Metrics, Results, Decision, Artifacts (templates, scripts)

7) Pattern Library governance

- Quarterly: Patterns are tagged Keep / Refine / Retire, with stewards assigned

### Pattern Library (examples)

- Welcome Wagon — increases 30‑day returns by pairing newcomers with a named host; use when first‑visit drop‑off >50%
- Repair First — require a repair ask before ejection talk; use when conflict tickets rise or trust dips
- Badge Night Stories — issue badges only after a witnessed contribution story; use when metrics start driving behavior


## Instrumentation: The Health Compass tie‑in
We use a small set of consistent signals across Self → Flotilla → Hub → Federation. Examples:

- Belonging (1–5)
- Reciprocity (1–5)
- Conflict Repair Success (0/1 per conflict instance, aggregated)
- Contribution Velocity (tasks/week normalized by group size)
- Fertility Desire vs Reality Gap (survey: desired children vs actual/expected, anonymized)

## Cross‑Hub A/B Governance

- Annual Festival: each Hub brings two “best experiments” and one “failed but instructive.”
- Round‑tables compare methods; publish a Federation Synthesis: what scaled, what backfired, where context mattered.

## Risks and Mitigations

- Goodhart’s law: over‑focus on numbers → pair metrics with qualitative debriefs and wisdom councils.
- Privacy: default to opt‑in, minimize data, publish only aggregates with clear k‑anonymity thresholds.
- Fatigue: cap concurrent experiments; rotate stewards; celebrate retirements as learning wins.
